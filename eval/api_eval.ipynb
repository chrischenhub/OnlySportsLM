{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from anthropic import Anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the inference outcome of the models you want to compare here, they will be combined into one big jsonl file. You can put as many as 7 model outcomes for LLMs to evaluate.\n",
    "file_names = ['modelA_output.jsonl', 'modelB_output.jsonl','modelC_output']\n",
    "\n",
    "# The new JSONL file to write the combined responses\n",
    "output_file = 'combined_outcome.jsonl'\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "    file_handles = [open(file_name, 'r', encoding='utf-8') for file_name in file_names]\n",
    "    for lines in zip(*file_handles):\n",
    "        responses = []\n",
    "        for line in lines:\n",
    "            entry = json.loads(line)\n",
    "            response = entry.get('response')\n",
    "            prompt = entry.get('prompt')\n",
    "            if response:\n",
    "                responses.append('[SEP]'+response)\n",
    "\n",
    "        output_data = {\"prompt\": prompt, \"response\": \" \".join(responses)}\n",
    "\n",
    "        output_f.write(json.dumps(output_data)+ \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_api = ''\n",
    "def claude_accuracy_response(prompt):\n",
    "    client = Anthropic(api_key=claude_api)\n",
    "   \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        system=\"\"\"You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. \n",
    "        One prompt and five responses will be presented, all attempting to complete the same given prompt. Each response is seperated by [SEP] and limited to 80 tokens.\n",
    "\n",
    "        Evaluate responses using the following rubric for \"Accuracy and Factuality\":\n",
    "                    \"1\": \"Mostly inaccurate, significant factual errors.\",\n",
    "                    \"2\": \"Partially accurate, mix of correct and incorrect information.\",\n",
    "                    \"3\": \"Mostly accurate, minor factual errors.\",\n",
    "                    \"4\": \"Highly accurate, negligible errors.\",\n",
    "                    \"5\": \"Fully accurate and factually impeccable.\"\n",
    "\n",
    "        When evaluating, only consider how well the generated text continues or extends the given prompt in terms of context, topic, and style.\n",
    "        Score these generated responses on a scale of 1-5. Only output the scores! Output scores in the following format: 'X, X, X, X, X', where X is a number between 1 and 5.\"\"\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_str = response.content[0].text.strip()\n",
    "    return response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def claude_relevance_response(prompt):\n",
    "    client = Anthropic(api_key=claude_api)\n",
    "\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=20,\n",
    "        temperature=0,\n",
    "        system=\"\"\"You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. \n",
    "        One prompt and five responses will be presented, all attempting to complete the same given prompt. Each response is seperated by [SEP] and limited to 80 tokens.\n",
    "\n",
    "        Evaluate responses using the following rubric for \"Continuity and Relevance\":\n",
    "            \"1\": \"Poor continuation, diverges significantly from the prompt's context or topic.\",\n",
    "            \"2\": \"Weak continuation, maintains some elements of the prompt but introduces unrelated content.\",\n",
    "            \"3\": \"Adequate continuation, generally follows the prompt's direction with some minor deviations.\",\n",
    "            \"4\": \"Strong continuation, closely follows the prompt's context and style with minimal inconsistencies.\",\n",
    "            \"5\": \"Excellent continuation, seamlessly extends the prompt's narrative, context, and style.\"\n",
    "\n",
    "        When evaluating, only consider how well the generated text continues or extends the given prompt in terms of context, topic, and style.\n",
    "        Score these generated responses on a scale of 1-5. Only output the scores! Output scores in the following format: 'X, X, X, X, X', where X is a number between 1 and 5.\"\"\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_str = response.content[0].text.strip()\n",
    "    return response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def OpenAI_relevence_response(client,prompt,num):\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"\"\"\n",
    "                You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. \n",
    "                1 prompt and {num} responses will be presented, all attempting to complete the same given prompt. Each response is seperated by [SEP] and limited to 80 tokens.\n",
    "\n",
    "                Evaluate responses using the following rubric for \"Continuity and Relevance\":\n",
    "                    \"1\": \"Poor continuation, diverges significantly from the prompt's context or topic.\",\n",
    "                    \"2\": \"Weak continuation, maintains some elements of the prompt but introduces unrelated content.\",\n",
    "                    \"3\": \"Adequate continuation, generally follows the prompt's direction with some minor deviations.\",\n",
    "                    \"4\": \"Strong continuation, closely follows the prompt's context and style with minimal inconsistencies.\",\n",
    "                    \"5\": \"Excellent continuation, seamlessly extends the prompt's narrative, context, and style.\"\n",
    "\n",
    "                When evaluating, only consider how well the generated text continues or extends the given prompt in terms of context, topic, and style.\n",
    "                Score these generated responses on a scale of 1-5. Only output the scores! Output scores in the following format: 'X, X, X, X, X, X...', where X is a number between 1 and 5.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0\n",
    "    )\n",
    "    ResponseStr = response.choices[0].message.content.strip()\n",
    "    return ResponseStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def OpenAI_accuracy_response(client,prompt,num):\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o\",\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"\"\"\n",
    "                You are a sports expert assigned to grade language models' generation performance on general sports-related text according to the provided rubric. \n",
    "                1 prompt and {num} responses will be presented, all attempting to complete the same given prompt. Each response is seperated by [SEP] and limited to 80 tokens.\n",
    "\n",
    "                Evaluate responses using the following rubric for \"Accuracy and Factuality\":\n",
    "                    \"1\": \"Mostly inaccurate, significant factual errors.\",\n",
    "                    \"2\": \"Partially accurate, mix of correct and incorrect information.\",\n",
    "                    \"3\": \"Mostly accurate, minor factual errors.\",\n",
    "                    \"4\": \"Highly accurate, negligible errors.\",\n",
    "                    \"5\": \"Fully accurate and factually impeccable.\"\n",
    "\n",
    "                When evaluating, only consider the accuracy and factuality in the context of the given prompt.\n",
    "                Score these generated responses on a scale of 1-5. Only output the scores! Output scores in the following format: 'X, X, X, X, X, X...', where X is a number between 1 and 5.\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0\n",
    "    )\n",
    "    ResponseStr = response.choices[0].message.content.strip()\n",
    "    return ResponseStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key='')\n",
    "counter =0\n",
    "scores_acc = []\n",
    "scores_rel = []\n",
    "counter=0\n",
    "with open('combined_outcome.jsonl', 'r', encoding='utf-8') as f:\n",
    "    response_num = 8\n",
    "    for line in f:\n",
    "        entry = json.loads(line)\n",
    "        counter+=1\n",
    "        res = (f\"\"\"prompt: {entry['prompt']} \\nResponse: {entry['response']}\"\"\")\n",
    "        score_acc = OpenAI_accuracy_response(client,res,response_num).split(\",\")\n",
    "        score_rel = OpenAI_relevence_response(client,res,response_num).split(\",\")\n",
    "        if len(score_acc)==response_num:\n",
    "            scores_acc.append(score_acc)\n",
    "        if len(score_rel)==response_num:\n",
    "            scores_rel.append(score_rel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
